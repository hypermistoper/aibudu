---
title: "Pay Attention to MLPs - Paper Explained"
image: "https:\/\/i.ytimg.com\/vi\/1932kS2Jq1M\/hqdefault.jpg"
vid_id: "1932kS2Jq1M"
categories: "Science-Technology"
tags: ["pay attention to mlps explained","gMLP","Spatial gating unit"]
date: "2022-03-20T18:38:58+03:00"
vid_date: "2021-06-27T13:30:01Z"
duration: "PT7M8S"
viewcount: "972"
likeCount: "37"
dislikeCount: ""
channel: "Halfling Wizard"
---
{% raw %}In this video, I explain the paper “Pay Attention to MLPs”. This article claims that self-attention blocks used in transformer architecture aren't necessary in many applications, and offers the gMLP architecture, which delivers outcomes comparable to transformers without using attention. In addition, the article suggests that adding a small single-head self-attention into gMLP will result in an even better architecture called aMLP.<br /><br />📑 Chapters:<br />0:00 Abstract<br />0:25 Introduction<br />1:37 Model (gMLP)<br />4:13 Results (gMLP)<br />5:46 Model (aMLP)<br />6:03 Resutls (aMLP)<br />6:16 Conclusion<br /><br />📝 Link to the paper:<br /><a rel="nofollow" target="blank" href="https://arxiv.org/abs/2105.08050">https://arxiv.org/abs/2105.08050</a><br /><br />👥 Authors:<br />Hanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le.<br />🔗 Helpful Links:<br />- My Video on the Paper &quot;Attention is All you Need&quot;<br /><a rel="nofollow" target="blank" href="https://www.youtube.com/watch?v=XowwKOAWYoQ">https://www.youtube.com/watch?v=XowwKOAWYoQ</a><br />- My Video on the Paper &quot;An Image Is Worth 16x16 Words&quot;<br /><a rel="nofollow" target="blank" href="https://www.youtube.com/watch?v=G7i330EJAfU">https://www.youtube.com/watch?v=G7i330EJAfU</a><br /><br />🙋‍♂️ Find me on:<br />- Find me on: <a rel="nofollow" target="blank" href="https://linktr.ee/HalflingWizard">https://linktr.ee/HalflingWizard</a><br /><br />#computer_vision #NLP #MLP #gMLP{% endraw %}
