---
title: "Flamingo: a Visual Language Model for Few-Shot Learning"
image: "https:\/\/i.ytimg.com\/vi\/H82s6BrJduM\/hqdefault.jpg"
vid_id: "H82s6BrJduM"
categories: "People-Blogs"
tags: ["Flamingo:","Visual","Language"]
date: "2022-06-09T00:39:59+03:00"
vid_date: "2022-05-08T18:28:33Z"
duration: "PT1H35M38S"
viewcount: "1368"
likeCount: "49"
dislikeCount: ""
channel: "Samuel Albanie"
---
{% raw %}This video provides a digest of the Flamingo model, introduced in the work &quot;Flamingo: a Visual Language Model for Few-Shot Learning&quot; by J-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc , A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, K. Simonyan, shared on arxiv in April 2022.<br /><br />Timestamps:<br />00:00 - Flamingo: a Visual Language Model for Few-Shot Learning<br />01:05 - Motivation<br />07:34 - Related work<br />14:14 - Flamingo model<br />41:43 - Evaluation datasets<br />50:06 - Comparison to state-of-the-art<br />01:01:52 - Ablations<br />01:12:41 - Qualitative Examples<br />01:22:29 - Discussion (limitations, trade-offs, opportunities, benefits)<br /><br />Particular thanks to Antoine Miech for his help in clarifying several details of the work.<br /><br />Slides (pdf): <a rel="nofollow" target="blank" href="https://samuelalbanie.com/files/digest-slides/2022-05-flamingo.pdf">https://samuelalbanie.com/files/digest-slides/2022-05-flamingo.pdf</a><br />The paper can be found on arxiv here: <a rel="nofollow" target="blank" href="https://arxiv.org/abs/2204.14198">https://arxiv.org/abs/2204.14198</a><br /><br />Papers mentioned in the video that also fit within the YouTube character limit (a complete list can be found at <a rel="nofollow" target="blank" href="http://samuelalbanie.com/digests/2022-05-flamingo/">http://samuelalbanie.com/digests/2022-05-flamingo/</a> ):<br />E. Markman, &quot;Categorization and naming in children: Problems of induction&quot;, MIT Press (1989)<br />S. Hochreiter et al., &quot;Long short-term memory&quot;, Neural computation (1997)<br />D. P. Kingma and J. Ba, &quot;Adam: A method for stochastic optimization&quot;, ICLR (2015)<br />J. Ba et al., &quot;Layer normalization&quot;, arxiv (2016)<br />I. Loshchilov and F. Hutter, &quot;Decoupled weight decay regularization&quot;, arxiv (2017)<br />A. Vaswani et al., &quot;Attention is all you need&quot;, NeurIPS (2017)<br />J. Bradbury et al., &quot;JAX: composable transformations of Python+ NumPy programs&quot; (2018)<br />E. Strubell et al., &quot;Energy and policy considerations for deep learning in NLP&quot;, ACL (2019)<br />T. L. Griffiths, et al., &quot;Doing more with less: meta-reasoning and meta-learning in humans and machines&quot;, Current Opinion in Behavioral Sciences (2019)<br />A. Radford et al., &quot;Language models are unsupervised multitask learners&quot;, (2019)<br />J. Devlin et al. &quot;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&quot;, NAACL-HLT (2019)<br />M. Mitchell et al., &quot;Model cards for model reporting&quot;, ACM FAccT (2019)<br />M. Shoeybi et al., &quot;Megatron-lm: Training multi-billion parameter language models using model parallelism&quot;, arxiv (2019)<br />C. Raffel, et al., &quot;Exploring the limits of transfer learning with a unified text-to-text transformer&quot;, JMLR (2019)<br />T. Brown et al., &quot;Language models are few-shot learners&quot;, NeurIPS (2020)<br />T. Hennigan et al., &quot;Haiku: Sonnet for Jax&quot; (2020)<br />N. Carion et al., &quot;End-to-end object detection with transformers&quot;, ECCV (2020)<br />S. Rajbhandari et al., &quot;Zero: Memory optimizations toward training trillion parameter models&quot;, SC (2020)<br />H. Touvron et al., &quot;Fixing the train-test resolution discrepancy: FixEfficientNet&quot;, arxiv (2020)<br />J. Kaplan et al., &quot;Scaling laws for neural language models&quot;, arxiv (2020)<br />A. Radford et al., &quot;Learning transferable visual models from natural language supervision&quot;, ICML (2021)<br />C. Jia et al., &quot;Scaling up visual and vision-language representation learning with noisy text supervision&quot;, ICML (2021)<br />J. Cho et al., &quot;Unifying vision-and-language tasks via text generation&quot;, ICML (2021)<br />A. Jaegle et al., &quot;Perceiver: General perception with iterative attention&quot;, ICML (2021)<br />R. Mokady et al., &quot;Clipcap: Clip prefix for image captioning&quot;, arxiv (2021)<br />M. Tsimpoukelli et al., &quot;Multimodal few-shot learning with frozen language models&quot;, NeurIPS (2021) <br />J. Rae, et al. &quot;Scaling language models: Methods, analysis &amp; insights from training gopher&quot;, arxiv (2021)<br />T. Gebru et al., &quot;Datasheets for datasets&quot;, Communications of the ACM (2021)<br />E. Perez et al., &quot;Red teaming language models with language models&quot;, arxiv (2022)<br />E. Perez et al., &quot;True few-shot learning with language models&quot;, NeurIPS (2021)<br />J. Liu et al., &quot;What Makes Good In-Context Examples for GPT-$3?&quot;, arxiv (2021)<br />Z. Yang et al., &quot;An empirical study of gpt-3 for few-shot knowledge-based vqa&quot;, arxiv (2021)<br />Z. Zhao et al., &quot;Calibrate before use: Improving few-shot performance of language models&quot;, ICML (2021)<br />A. Brock et al., &quot;High-performance large-scale image recognition without normalization&quot;, ICML (2021)<br />Z. Wang et al., &quot;Simvlm: Simple visual language model pretraining with weak supervision&quot;, arxiv (2021)<br />L. Reynolds and K. McDonell, &quot;Prompt programming for large language models: Beyond the few-shot paradigm&quot;, CHI (2021)<br />X. Zhai et al., &quot;Scaling vision transformers&quot;, arxiv (2021)<br />J. Hoffmann et al., &quot;Training Compute-Optimal Large Language Models&quot;, arxiv (2022)<br />A. Aghajanyan et al., &quot;CM3: A Causal Masked Multimodal Model of the Internet&quot;, arxiv (2022)<br />S. Min et al., &quot;Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?&quot;, arxiv (2022)<br />J-B. Alayrac et al., &quot;Flamingo: a Visual Language Model for Few-Shot Learning&quot;, arxiv (2022){% endraw %}
